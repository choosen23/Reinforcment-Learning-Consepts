{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6c4c45",
   "metadata": {},
   "source": [
    "# The multiarmed bandit problem\n",
    "We are tring to maximize the reward of the bandid problem or to minimize the regret of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28a418a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source env/bin/activate\n",
    "# !pip install numpy\n",
    "# !pip isntall pandas\n",
    "# !pip install random\n",
    "# !pip install math\n",
    "# !pip install scipy\n",
    "import env\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4ad9e6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98114589]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "r = beta(20, 1)\n",
    "print(r.rvs(size=1))\n",
    "ax.hist(r.rvs(size=1000), density=True, histtype='stepfilled', alpha=0.2)\n",
    "ax.legend(loc='best', frameon=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "542f498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "    def createRewards(self,totalTime):\n",
    "        \"\"\"\n",
    "        Function to create rewards base on the simulation time using bernoulli \n",
    "        distribution\n",
    "        \"\"\"\n",
    "        rewardPercentage = 0.45\n",
    "        lossPercentage = 0.55\n",
    "        return list(bernoulli.rvs(rewardPercentage, lossPercentage, totalTime))\n",
    "\n",
    "\n",
    "    def __init__(self,id,totalTime):\n",
    "        \"\"\"\n",
    "        Initialize the bandids name, time of play and the range the rewards will\n",
    "        get.\n",
    "    \n",
    "        \"\"\"\n",
    "        self.distrParameters = (1,1)\n",
    "        self.id = id\n",
    "        self.listCurrent = self.createRewards(totalTime)\n",
    "        self.listPlayed = []\n",
    "        # print(self.listCurrent)\n",
    "  \n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def nextstateBandit(self,timestamp):\n",
    "        \"\"\"\n",
    "        Next state will give a spin the bandit and will give us\n",
    "        a reward from the listCurrent. Also this reward will go to the listPLayed \n",
    "        states.\n",
    "\n",
    "        \"\"\"\n",
    "        reward = self.listCurrent.pop(0)\n",
    "        self.listPlayed.append(reward)\n",
    "        return reward\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "be2d6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Auxilary functions for plotting\n",
    "\"\"\"    \n",
    "\n",
    "def plotBetaFunctions(listofBandits,time):\n",
    "    \"\"\"\n",
    "    For each step plot the distributions of each bandit\n",
    "    \"\"\"\n",
    "    fig , axs = plt.subplots(nrows=len(listofBandits),ncols=1, figsize=(15, 12))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    " \n",
    "\n",
    "    for i in range(len(listofBandits)):\n",
    "        id =  listofBandits[i].id\n",
    "        pr = listofBandits[i].distrParameters\n",
    "        r = beta.rvs(pr[0],pr[1],size=1000)\n",
    "        axs[i].hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
    "        fig.suptitle(f\"Bandid {id} for time {time}\")\n",
    "        fig.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "9f6ea6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    Initialize the agent. We determine the number of bandits, the total time of the\n",
    "    simulation and the range of rewards they will give. Also we initialize the action \n",
    "    space and some functions for the rewards. Also we use the approach method in order \n",
    "    too compute whatever we want.\n",
    "\n",
    "    actionState : (id, time, reward)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,totalTime,numOfBandits,approach):\n",
    "        self.totalTime = int(totalTime)\n",
    "        self.approach = approach\n",
    "        self.numOfBandits = int(numOfBandits)\n",
    "        self.actionState = []\n",
    "        self.listOfBandits = []  \n",
    "        for bandit in range(self.numOfBandits):\n",
    "            self.listOfBandits.append(Bandit(bandit,self.totalTime))\n",
    "\n",
    "        return None\n",
    "        \n",
    "\n",
    "\n",
    "    def countofAction(self,id):\n",
    "        \"\"\"\n",
    "        Function which computes the count of action with 'id' == id\n",
    "        \"\"\"\n",
    "        indicator = 0\n",
    "        for i in self.actionState:\n",
    "            if i[0] == id:\n",
    "                indicator += 1\n",
    "        return indicator\n",
    "\n",
    "\n",
    "    def nextState(self,timestep,bandit):\n",
    "        \"\"\"\n",
    "        Function where the agent goes to next stage :\n",
    "            t -> t + 1\n",
    "\n",
    "        We take as an input the bandit we will pull and the timestamp\n",
    "        and we return the last action.\n",
    "            \n",
    "        \"\"\"\n",
    "        #we going to next state\n",
    "        currentBandid = self.listOfBandits[bandit]\n",
    "        currentBandidReward = currentBandid.nextstateBandit(timestep)\n",
    "\n",
    "        if approach in ['bayesian','thompsonSampling']:\n",
    "            if currentBandidReward == 0:\n",
    "                currentBandid.distrParameters = (currentBandid.distrParameters[0]\n",
    "                                                , currentBandid.distrParameters[1]+1) \n",
    "\n",
    "            else:\n",
    "                currentBandid.distrParameters = (currentBandid.distrParameters[0]+1\n",
    "                                                , currentBandid.distrParameters[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"timestep {timestep} , reward {currentBandidReward}\")\n",
    "        self.actionState.append((bandit,timestep,currentBandidReward))\n",
    "        return self.actionState[-1]\n",
    "\n",
    "\n",
    "    def currentReward(self):\n",
    "        \"\"\"\n",
    "        Function which culculated the reward at the last moment\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        for i in self.actionState:\n",
    "            reward += i[2]\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def averageOfSampledRewards(self,id):\n",
    "        \"\"\"\n",
    "        Cuclulates the average of sampled rewards for each bandit\n",
    "        \"\"\"\n",
    "        upper = 0\n",
    "        lower = 0\n",
    "        for i in self.actionState:\n",
    "            if id == i[0]:\n",
    "                upper += i[2]\n",
    "                lower += 1\n",
    "        if lower == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return upper/lower\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def regeretOFAction(self,id):\n",
    "        \"\"\"\n",
    "        Regret of an action a is:\n",
    "\n",
    "            D = v* - q(a)\n",
    "\n",
    "        where :\n",
    "            * v* is the maximum expected reward \n",
    "            * q(a) is the value of action a\n",
    "\n",
    "        \"\"\"\n",
    "        #parameters\n",
    "        V = 1\n",
    "        q_a = self.averageOfSampledRewards(id)\n",
    "        return V - q_a\n",
    "\n",
    "\n",
    "\n",
    "    def actionValue(self,a):\n",
    "        \"\"\"\n",
    "        Action value: The expected reward for action a\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def randomSelection(self,timestep):\n",
    "        \"\"\"\n",
    "        This function selects randomly a bandit and returns the new state to\n",
    "        the action stages.\n",
    "\n",
    "        \"\"\"\n",
    "        currentbanditId = random.randint(0,self.numOfBandits-1)\n",
    "        currentBandid = self.listOfBandits[currentbanditId]\n",
    "        currentBandidReward = currentBandid.nextstateBandit(timestep)\n",
    "        if approach in ['bayesian','thompsonSampling']:\n",
    "            if currentBandidReward == 0:\n",
    "                currentBandid.distrParameters = (currentBandid.distrParameters[0]\n",
    "                                                , currentBandid.distrParameters[1]+1) \n",
    "\n",
    "            else:\n",
    "                currentBandid.distrParameters = (currentBandid.distrParameters[0]+1\n",
    "                                                , currentBandid.distrParameters[1])\n",
    "\n",
    "        state = (currentbanditId, timestep, currentBandidReward)\n",
    "        self.actionState.append(state)\n",
    "        return state\n",
    "\n",
    "    def greedy(self,timestep):\n",
    "        \"\"\"\n",
    "        Greedy function for the next state. At every steps it selectst the\n",
    "        next bandit based on the bigger Average Sample Rewards\n",
    "\n",
    "        \"\"\"\n",
    "        #we select our bandit\n",
    "        maxReward = -1\n",
    "        maxBandit = -1\n",
    "        for i in self.listOfBandits:\n",
    "            currentsampleReward = self.averageOfSampledRewards(i.id)\n",
    "            if currentsampleReward > maxReward:\n",
    "                maxReward = currentsampleReward\n",
    "                maxBandit = i.id\n",
    "        \n",
    "        return self.nextState(timestep,maxBandit)\n",
    "\n",
    "\n",
    "\n",
    "    def epsilonGreedy(self,timestep):\n",
    "        \"\"\"\n",
    "        eGreedy:\n",
    "            with propability 1-e selects greedy action\n",
    "            with propability e selects a random action\n",
    "        \"\"\"\n",
    "        epsilon = 0.1\n",
    "        value = random.random()\n",
    "        if value > epsilon:\n",
    "            result = self.greedy(timestep)\n",
    "        else:\n",
    "            result = self.randomSelection(timestep)\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    def uppercoefidenceboundValue(self,timestep,id):\n",
    "        \"\"\"\n",
    "        Culculates the upper coefidence bound for the given id.\n",
    "\n",
    "        \"\"\"\n",
    "        c = math.sqrt(2)\n",
    "        if self.countofAction(id) > 0:\n",
    "            return c*math.sqrt( math.log10(timestep) / self.countofAction(id) )\n",
    "        else : \n",
    "            return 0\n",
    "\n",
    "    def UpperConfidenceBounds(self,timestep):\n",
    "        \"\"\"\n",
    "        Upper Confidence Bound:\n",
    "        It is using a factor where it hepls to decide better.\n",
    "\n",
    "        \"\"\"\n",
    "        #we select our bandit\n",
    "        maxReward = 0\n",
    "        maxBandit = 0\n",
    "        upperBound = self.uppercoefidenceboundValue(timestep,0)\n",
    "\n",
    "        for i in self.listOfBandits:\n",
    "            upperBound = self.uppercoefidenceboundValue(timestep,i.id)\n",
    "            currentsampleReward = self.averageOfSampledRewards(i.id) + upperBound\n",
    "            # print(f\"timestep {timestep} , reward {currentsampleReward}\")\n",
    "            if currentsampleReward > maxReward:\n",
    "                maxReward = currentsampleReward\n",
    "                maxBandit = i.id\n",
    "        \n",
    "\n",
    "        return self.nextState(timestep,maxBandit)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def actionPreferences():\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def policySearch():\n",
    "        pass\n",
    "\n",
    "\n",
    "    def upperConfidenceBoundBayesian(self,time,id):\n",
    "        \"\"\"\n",
    "        The ucb for the bayesian approach will be the standar deviation of the Beta \n",
    "         distribution multiplied with a constant.\n",
    "\n",
    "            U(q) = c*sigma(Ut)\n",
    "\n",
    "        \"\"\"\n",
    "        c = np.sqrt(2)\n",
    "        pr = self.listOfBandits[id].distrParameters\n",
    "        return c*beta(pr[0],pr[1]).var()\n",
    "\n",
    "\n",
    "\n",
    "    def bayesianApproach(self,timestep):\n",
    "        \"\"\"\n",
    "        Bayesian approach to the bandit problem :\n",
    "            * For each action we think each value is in [0,1] equallly likely (prior ~ U(0,1))\n",
    "            * The posterior is a Beta Distribution(xa,ya) where initially the xa=ya=1 and:\n",
    "            xa <- xa + 1 , whene Rt = 0\n",
    "            ya <- ya + 1 , where Rt = 1\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #we select our bandit\n",
    "        maxReward = 0\n",
    "        maxBandit = 0\n",
    "\n",
    "        for i in self.listOfBandits:\n",
    "            upperBound = self.upperConfidenceBoundBayesian(timestep,i.id)\n",
    "            currentsampleReward = self.averageOfSampledRewards(i.id) + upperBound\n",
    "            # currentsampleReward = upperBound\n",
    "\n",
    "            print(f\"timestep {timestep} , reward {currentsampleReward}, bandit {i.id}\")\n",
    "            if currentsampleReward > maxReward:\n",
    "                maxReward = currentsampleReward\n",
    "                maxBandit = i.id\n",
    "        \n",
    "        return self.nextState(timestep,maxBandit)\n",
    "\n",
    "         \n",
    "    def thompsonPropability(self,id):\n",
    "        \"\"\"\n",
    "        Function to culculates the Thompson variable to decide which\n",
    "        bandit we are going to use next.\n",
    "\n",
    "        \"\"\"\n",
    "        return beta.rvs(id[0],id[1],size=1)\n",
    "\n",
    "\n",
    "\n",
    "    def thompsonSampling(self,timestep):\n",
    "        \"\"\"\n",
    "        At Thompson Sampling we begin with the prior distribution beta(1,1) ~ Uniform\n",
    "        Then randomly we select a number from the Beta from each bandit. If is 1 then we \n",
    "        update the upper bound else, we update the lower. For each timestep we follow the same\n",
    "        procedure.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        maxReward = 0\n",
    "        maxBandit = 0\n",
    "\n",
    "        for i in self.listOfBandits:\n",
    "            thompsonValue = self.thompsonPropability(i.distrParameters)\n",
    "            # print(f\"timestep {timestep} , reward {thompsonValue}, bandit {i.id}\")\n",
    "            if thompsonValue > maxReward:\n",
    "                maxReward = thompsonValue\n",
    "                maxBandit = i.id\n",
    "        \n",
    "        return self.nextState(timestep,maxBandit)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def nextStateAgent(self,timestep,approach):\n",
    "        \"\"\"\n",
    "        Function which lead our agent to the next state.\n",
    "\n",
    "        If is the first step of the agent we randomly select a bandit.\n",
    "\n",
    "        Else:\n",
    "            for each bandid we culculate the next step based on the approach we \n",
    "            want to examine.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if timestep == 0:\n",
    "            return self.randomSelection(timestep)\n",
    "        else:\n",
    "            if self.approach == 'greedy':\n",
    "                return self.greedy(timestep)\n",
    "\n",
    "            if self.approach =='egreedy':\n",
    "                return self.epsilonGreedy(timestep)\n",
    "                        \n",
    "            if self.approach == 'ucb':\n",
    "                return self.UpperConfidenceBounds(timestep)\n",
    "        \n",
    "            if self.approach == 'bayesian':\n",
    "                # plotBetaFunctions(self.listOfBandits,timestep)\n",
    "                return self.bayesianApproach(timestep)\n",
    "\n",
    "            if self.approach == 'thompsonSampling':\n",
    "                return self.thompsonSampling(timestep)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "bd79996f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 0, 1)\n",
      "(2, 1, 1)\n",
      "(0, 2, 1)\n",
      "(2, 3, 1)\n",
      "(4, 4, 0)\n",
      "(0, 5, 1)\n",
      "(3, 6, 1)\n",
      "(1, 7, 0)\n",
      "(0, 8, 1)\n",
      "(3, 9, 0)\n",
      "(0, 10, 0)\n",
      "(2, 11, 0)\n",
      "(0, 12, 1)\n",
      "(0, 13, 1)\n",
      "(4, 14, 0)\n",
      "(3, 15, 1)\n",
      "(3, 16, 0)\n",
      "(0, 17, 0)\n",
      "(2, 18, 0)\n",
      "(2, 19, 1)\n",
      "(1, 20, 0)\n",
      "(2, 21, 1)\n",
      "(3, 22, 0)\n",
      "(2, 23, 1)\n",
      "(3, 24, 1)\n",
      "(2, 25, 0)\n",
      "(4, 26, 0)\n",
      "(2, 27, 1)\n",
      "(3, 28, 0)\n",
      "(2, 29, 0)\n",
      "(0, 30, 1)\n",
      "(0, 31, 0)\n",
      "(3, 32, 1)\n",
      "(2, 33, 0)\n",
      "(0, 34, 1)\n",
      "(2, 35, 0)\n",
      "(0, 36, 0)\n",
      "(0, 37, 1)\n",
      "(0, 38, 1)\n",
      "(2, 39, 1)\n",
      "(0, 40, 0)\n",
      "(0, 41, 0)\n",
      "(0, 42, 1)\n",
      "(4, 43, 0)\n",
      "(3, 44, 1)\n",
      "(1, 45, 0)\n",
      "(0, 46, 1)\n",
      "(3, 47, 1)\n",
      "(0, 48, 0)\n",
      "(3, 49, 0)\n",
      "(2, 50, 0)\n",
      "(2, 51, 0)\n",
      "(0, 52, 1)\n",
      "(0, 53, 0)\n",
      "(3, 54, 0)\n",
      "(3, 55, 0)\n",
      "(0, 56, 1)\n",
      "(0, 57, 0)\n",
      "(2, 58, 0)\n",
      "(0, 59, 0)\n",
      "(0, 60, 1)\n",
      "(3, 61, 1)\n",
      "(0, 62, 0)\n",
      "(3, 63, 1)\n",
      "(2, 64, 0)\n",
      "(2, 65, 1)\n",
      "(1, 66, 1)\n",
      "(2, 67, 1)\n",
      "(2, 68, 0)\n",
      "(0, 69, 0)\n",
      "(2, 70, 0)\n",
      "(0, 71, 0)\n",
      "(2, 72, 1)\n",
      "(3, 73, 0)\n",
      "(2, 74, 1)\n",
      "(2, 75, 0)\n",
      "(2, 76, 0)\n",
      "(3, 77, 1)\n",
      "(2, 78, 0)\n",
      "(0, 79, 0)\n",
      "(0, 80, 0)\n",
      "(0, 81, 0)\n",
      "(3, 82, 1)\n",
      "(4, 83, 1)\n",
      "(1, 84, 1)\n",
      "(2, 85, 1)\n",
      "(0, 86, 1)\n",
      "(3, 87, 1)\n",
      "(3, 88, 1)\n",
      "(3, 89, 1)\n",
      "(3, 90, 0)\n",
      "(0, 91, 0)\n",
      "(0, 92, 0)\n",
      "(3, 93, 0)\n",
      "(2, 94, 0)\n",
      "(3, 95, 1)\n",
      "(2, 96, 1)\n",
      "(4, 97, 1)\n",
      "(1, 98, 0)\n",
      "(3, 99, 0)\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "possibleApproaches = ['greedy','egreedy','ucb','bayesian']\n",
    "\n",
    "totalTime = 100\n",
    "bandits = 5\n",
    "approach = 'thompsonSampling'\n",
    "states = []\n",
    "\n",
    "\n",
    "\n",
    "agent =  Agent(totalTime,bandits,approach)\n",
    "for t in range(totalTime):\n",
    "    states.append(agent.nextStateAgent(t,approach))\n",
    "    \n",
    "for i in states:\n",
    "    print(i)\n",
    "print(agent.currentReward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ccb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fafbaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
